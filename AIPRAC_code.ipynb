{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy\n",
    "import pandas\n",
    "import matplotlib\n",
    "import seaborn\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, precision_recall_curve\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deal Status</th>\n",
       "      <th>Payment Type</th>\n",
       "      <th>Announced Value</th>\n",
       "      <th>Deal Attributes</th>\n",
       "      <th>Continent Similarity</th>\n",
       "      <th>Nature of Bid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>57240.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>30330.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>110</td>\n",
       "      <td>28985.65</td>\n",
       "      <td>100000000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>25924.59</td>\n",
       "      <td>100000000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>25097.70</td>\n",
       "      <td>100010000000</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Deal Status  Payment Type  Announced Value  Deal Attributes  \\\n",
       "0            0           100         57240.00                0   \n",
       "1            0           100         30330.00                0   \n",
       "2            1           110         28985.65     100000000000   \n",
       "3            0           110         25924.59     100000000000   \n",
       "4            0            -1         25097.70     100010000000   \n",
       "\n",
       "   Continent Similarity  Nature of Bid  \n",
       "0                     1             -1  \n",
       "1                     0             -1  \n",
       "2                     1             -1  \n",
       "3                     1             -1  \n",
       "4                     0             -1  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r'/Users/elee1596/Dropbox/Cornell/Junior First_Semester/CS 4700/AIPRAC_CS4701/status_output.csv')\n",
    "data.drop('Target Name', axis = 1, inplace = True)\n",
    "data.drop('Acquirer Name', axis = 1, inplace = True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Deal Status  Payment Type  Announced Value  Deal Attributes  \\\n",
      "count  4249.000000   4249.000000      4249.000000     4.249000e+03   \n",
      "mean      0.918098     90.001177       806.990104     4.254517e+10   \n",
      "std       0.274247     30.329666      1991.299428     4.999287e+10   \n",
      "min       0.000000     -1.000000       116.900000     0.000000e+00   \n",
      "25%       1.000000    100.000000       175.410000     0.000000e+00   \n",
      "50%       1.000000    100.000000       286.750000     1.010000e+09   \n",
      "75%       1.000000    100.000000       614.000000     1.000000e+11   \n",
      "max       1.000000    111.000000     57240.000000     1.111100e+11   \n",
      "\n",
      "       Continent Similarity  Nature of Bid  \n",
      "count           4249.000000    4249.000000  \n",
      "mean               0.740410      -0.983526  \n",
      "std                0.438462       0.149424  \n",
      "min                0.000000      -1.000000  \n",
      "25%                0.000000      -1.000000  \n",
      "50%                1.000000      -1.000000  \n",
      "75%                1.000000      -1.000000  \n",
      "max                1.000000       1.000000  \n"
     ]
    }
   ],
   "source": [
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape is: \n",
      "(4249, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data shape is: \")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data['Deal Status']\n",
    "X = data.drop('Deal Status', axis = 1)\n",
    "xTr, xTe, yTr, yTe = train_test_split(X, Y, test_size = 0.15, random_state = 123, stratify = Y)\n",
    "#where ytr == 1, over total amnt of points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(index = ['accuracy', 'precision', 'recall'],  columns = ['ClassTree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.9200626959247649\n",
      "\n",
      "Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.06      0.11        52\n",
      "          1       0.92      1.00      0.96       586\n",
      "\n",
      "avg / total       0.90      0.92      0.89       638\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 2. create instance of estimator \n",
    "class_tree = DecisionTreeClassifier(min_samples_split = 47, min_samples_leaf = 10, random_state = 10)\n",
    "# 3. use training data to train estimator\n",
    "class_tree.fit(xTr, yTr)\n",
    "# 4. evaluate model\n",
    "y_pred_test = class_tree.predict(xTe)\n",
    "\n",
    "metrics.loc['accuracy', 'ClassTree'] = accuracy_score(y_pred = y_pred_test, y_true = yTe)\n",
    "print(\"Accuracy is \" + str(metrics.loc['accuracy', 'ClassTree']))\n",
    "print(\"\")\n",
    "\n",
    "metrics.loc['precision', 'ClassTree'] = precision_score(y_pred = y_pred_test, y_true = yTe)\n",
    "print(\"Classification report: \")\n",
    "print(classification_report(yTe,y_pred_test.round()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.9184952978056427\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        52\n",
      "          1       0.92      1.00      0.96       586\n",
      "\n",
      "avg / total       0.84      0.92      0.88       638\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "NBC = GaussianNB()\n",
    "NBC.fit(xTr, yTr)\n",
    "y_pred_test = NBC.predict(xTe)\n",
    "metrics.loc['accuracy', 'ClassTree'] = accuracy_score(y_pred = y_pred_test, y_true = yTe)\n",
    "metrics.loc['precision', 'ClassTree'] = precision_score(y_pred = y_pred_test, y_true = yTe)\n",
    "print(\"Accuracy is \" + str(metrics.loc['accuracy', 'ClassTree']))\n",
    "print(\"\")\n",
    "\n",
    "preds = NBC.predict_proba(xTe)\n",
    "#print(\"Examples of predictions: \")\n",
    "#print(preds)\n",
    "print(\"\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(yTe,y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy is 0.9184952978056427\n",
      "\n",
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        52\n",
      "          1       0.92      1.00      0.96       586\n",
      "\n",
      "avg / total       0.84      0.92      0.88       638\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1232: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_regression = LogisticRegression(n_jobs = -1, random_state = 15)\n",
    "logistic_regression.fit(xTr, yTr)\n",
    "y_pred_test = logistic_regression.predict(xTe)\n",
    "preds = logistic_regression.decision_function(xTe)\n",
    "\n",
    "#print(\"Examples of Predictions:\")\n",
    "#print(logistic_regression.predict_proba(xTe))\n",
    "print(\"\")\n",
    "metrics.loc['accurary', 'ClassTree'] = accuracy_score(y_pred = y_pred_test, y_true = yTe)\n",
    "metrics.loc['precision', 'ClassTree'] = precision_score(y_pred = y_pred_test, y_true = yTe)\n",
    "print(\"Accuracy is \" + str(metrics.loc['accuracy', 'ClassTree']))\n",
    "print(\"\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(yTe,y_pred_test.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[  0  52]\n",
      " [  0 586]]\n",
      "\n",
      "Classification Report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        52\n",
      "          1       0.92      1.00      0.96       586\n",
      "\n",
      "avg / total       0.84      0.92      0.88       638\n",
      "\n",
      "\n",
      "Accuracy is 0.9184952978056427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regressor = RandomForestRegressor(n_estimators=15, random_state=1, max_features = .3, max_depth = 3)  \n",
    "regressor.fit(xTr, yTr)  \n",
    "y_pred = regressor.predict(xTe) \n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(\"Confusion Matrix: \")\n",
    "print(confusion_matrix(yTe,y_pred_test.round()))  \n",
    "print(\"\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(yTe,y_pred_test.round())) \n",
    "print(\"\")\n",
    "print(\"Accuracy is \" + str(metrics.loc['accuracy', 'ClassTree']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 20\n",
      "Accuracy is 0.9184952978056427\n",
      "Classification Report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.04      0.07        52\n",
      "          1       0.92      1.00      0.96       586\n",
      "\n",
      "avg / total       0.90      0.92      0.89       638\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report\n",
    "x = 20\n",
    "estimator = GradientBoostingClassifier(n_estimators=x, learning_rate=0.10, max_depth=3, random_state=0)\n",
    "GB = estimator.fit(xTr, yTr)\n",
    "y_pred = estimator.predict(xTe)\n",
    "print(\"n_estimators = \" + str(x))\n",
    "print(\"Accuracy is \" + str(metrics.loc['accuracy', 'ClassTree']))  \n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(yTe, y_pred))\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 60\n",
      "Accuracy is 0.9184952978056427\n",
      "Classification Report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        52\n",
      "          1       0.92      1.00      0.96       586\n",
      "\n",
      "avg / total       0.84      0.92      0.88       638\n",
      "\n",
      "\n",
      "\n",
      "n_estimators = 70\n",
      "Accuracy is 0.9184952978056427\n",
      "Classification Report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.02      0.04        52\n",
      "          1       0.92      1.00      0.96       586\n",
      "\n",
      "avg / total       0.89      0.92      0.88       638\n",
      "\n",
      "\n",
      "\n",
      "n_estimators = 80\n",
      "Accuracy is 0.9184952978056427\n",
      "Classification Report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.02      0.04        52\n",
      "          1       0.92      1.00      0.96       586\n",
      "\n",
      "avg / total       0.89      0.92      0.88       638\n",
      "\n",
      "\n",
      "\n",
      "n_estimators = 90\n",
      "Accuracy is 0.9184952978056427\n",
      "Classification Report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.02      0.04        52\n",
      "          1       0.92      1.00      0.96       586\n",
      "\n",
      "avg / total       0.89      0.92      0.88       638\n",
      "\n",
      "\n",
      "\n",
      "n_estimators = 100\n",
      "Accuracy is 0.9184952978056427\n",
      "Classification Report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.02      0.04        52\n",
      "          1       0.92      1.00      0.96       586\n",
      "\n",
      "avg / total       0.89      0.92      0.88       638\n",
      "\n",
      "\n",
      "\n",
      "n_estimators = 110\n",
      "Accuracy is 0.9184952978056427\n",
      "Classification Report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.10      0.17        52\n",
      "          1       0.93      1.00      0.96       586\n",
      "\n",
      "avg / total       0.92      0.92      0.90       638\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "x = 50\n",
    "while x <= 100:\n",
    "    AB = AdaBoostClassifier(base_estimator=None, n_estimators=x, learning_rate=.10, algorithm='SAMME.R', random_state=None)\n",
    "    adaboost = AB.fit(xTr, yTr)\n",
    "    y_pred = AB.predict(xTe)\n",
    "    x= x+10\n",
    "    print(\"n_estimators = \" + str(x))\n",
    "    print(\"Accuracy is \" + str(metrics.loc['accuracy', 'ClassTree']))\n",
    "    print(\"Classification Report: \")\n",
    "    print(classification_report(yTe, y_pred))\n",
    "    print(\"\")\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
